# 压缩实验

Owner: 柒柒在笔记

## 实验内容

本实验旨在对给定矩阵进行一阶熵估计，并应用霍夫曼编码进行数据压缩。通过计算熵值，可以了解数据的平均信息量，霍夫曼编码则是一种常用的无损压缩方法，通过变长编码实现压缩效果。通过实验，可以加深对信息论和编码理论的理解。

## 实验步骤

### 1. 定义矩阵

定义一个4x4的矩阵f，用于计算熵和进行霍夫曼编码。

```matlab
f = [119 123 168 119; 107 119 168 168; 119 119 107 119; 168 107 119 119];

```

在这一步中，我们定义了一个具体的矩阵作为研究对象，该矩阵包含了一系列的符号值，将用于后续的熵计算和编码。

### 2. 计算矩阵f的一阶熵估计

使用`entropy`函数计算矩阵f的一阶熵估计，结果以比特每符号(bits/symbol)为单位。

```matlab
h = entropy(f);
fprintf('矩阵f的一阶熵估计为: %f bits/symbol\n', h);
```

通过计算熵值，我们可以了解该矩阵数据的平均信息量，这对于理解数据的内在结构和复杂性非常重要。

### 3. 计算符号的概率分布

计算矩阵f中每个符号出现的概率分布。

```matlab
symbols = unique(f);
counts = histcounts(f(:), [symbols; max(symbols)+1] - 0.5);
p = counts / sum(counts);
```

在这一步中，通过计算每个符号出现的频率并归一化得到概率分布，这是进行霍夫曼编码的基础。

### 4. 进行霍夫曼编码

使用自定义的`huffman`函数对符号的概率分布进行霍夫曼编码。

```matlab
CODE = huffman(p);
```

通过霍夫曼编码，将符号根据其出现的概率进行编码，频率高的符号使用较短的编码，频率低的符号使用较长的编码，从而实现压缩。

### 5. 显示霍夫曼编码结果

输出每个符号对应的霍夫曼编码结果。

```matlab
disp('霍夫曼编码结果:');
for i = 1:length(symbols)
    fprintf('符号 %d 的编码: %s\n', symbols(i), CODE{i});
end
```

在这一步中，我们将每个符号的霍夫曼编码结果进行输出，以便查看编码结果和验证编码的正确性。

### 6. 实验代码

完整的实验代码如下：

```matlab
% 定义矩阵f
f = [119 123 168 119; 107 119 168 168; 119 119 107 119; 168 107 119 119];

% 计算矩阵f的一阶熵估计
h = entropy(f);
fprintf('矩阵f的一阶熵估计为: %f bits/symbol\n', h);

% 计算符号的概率分布
symbols = unique(f);
counts = histcounts(f(:), [symbols; max(symbols)+1] - 0.5);
p = counts / sum(counts);

% 进行霍夫曼编码
CODE = huffman(p);

% 显示霍夫曼编码结果
disp('霍夫曼编码结果:');
for i = 1:length(symbols)
    fprintf('符号 %d 的编码: %s\n', symbols(i), CODE{i});
end

% 计算矩阵的一阶熵估计
function h = entropy(x, n)
    if nargin < 2
       n = 256; % 如果未指定，使用默认的符号数
    end

    x = double(x); % 确保x是double类型
    edges = linspace(min(x(:))-0.5, max(x(:))+0.5, n+1);
    xh = histcounts(x(:), edges); % 计算n-bin直方图
    xh = xh / sum(xh(:)); % 计算概率分布
    i = find(xh); % 找到非零概率的索引
    h = -sum(xh(i) .* log2(xh(i))); % 计算熵
end

% 构建霍夫曼编码
function CODE = huffman(p)
    global CODE % 使用全局变量来存储编码结果
    CODE = cell(length(p), 1); % 初始化CODE为cell数组
    if length(p) > 1
       p = p / sum(p); % 归一化概率
       s = reduce(p); % 减少霍夫曼树
       makecode(s, []); % 递归生成编码
    else
       CODE = {'1'}; % 如果只有一个符号，编码为'1'
    end
end

% 减少霍夫曼树
function s = reduce(p)
    s = cell(length(p), 1);
    for i = 1:length(p)
       s{i} = i; % 初始化为符号索引
    end

    while numel(s) > 2 % 当剩余符号数大于2时继续合并
       [p, i] = sort(p); % 按概率排序
       p(2) = p(1) + p(2); % 合并最小的两个概率
       p(1) = [];
       s = s(i); % 按新概率排序树节点
       s{2} = {s{1}, s{2}}; % 合并节点
       s(1) = [];
    end
end

% 生成霍夫曼编码
function makecode(sc, codeword)
    global CODE
    if isa(sc, 'cell') % 如果是cell，表示非叶节点
       makecode(sc{1}, [codeword 0]); % 向左递归，添加0
       makecode(sc{2}, [codeword 1]); % 向右递归，添加1
    else % 如果是叶节点
       CODE{sc} = char('0' + codeword); % 将编码转换为字符
    end
end

```

## 实验结果

### 1. 矩阵f的一阶熵估计

计算得出矩阵f的一阶熵估计为:

```
矩阵f的一阶熵估计为: 1.930002 bits/symbol
```

通过这一结果，我了解到矩阵中符号的平均信息量，熵值越大，表示数据的随机性越大，压缩的潜力也越大。

### 2. 符号的概率分布

通过计算得到每个符号的概率分布如下：

```
符号 107 的概率: 0.1875
符号 119 的概率: 0.5
符号 123 的概率: 0.0625
符号 168 的概率: 0.25
```

这些概率分布用于霍夫曼编码，频率越高的符号将获得较短的编码，频率较低的符号将获得较长的编码。

### 3. 霍夫曼编码结果

通过霍夫曼编码，得到每个符号的编码结果：

```
霍夫曼编码结果:
符号 107 的编码: 10
符号 119 的编码: 0
符号 123 的编码: 110
符号 168 的编码: 111
```

通过霍夫曼编码，符号根据其出现频率被分配了不同长度的编码，达到压缩数据的目的。

## 总结

本实验通过计算矩阵f的一阶熵估计，并应用霍夫曼编码进行数据压缩，使我加深了对信息论和编码理论的理解。熵估计帮助我了解数据的平均信息量，而霍夫曼编码展示了如何通过变长编码实现数据压缩。这些知识对于数据压缩和传输中的实际应用具有重要意义。

通过本次实验，我对熵和霍夫曼编码的概念有了更深入的理解，特别是它们在数据压缩中的应用。实验结果展示了霍夫曼编码在不同符号概率分布下的编码效果，验证了其在实际数据压缩中的有效性。

## 参考文献